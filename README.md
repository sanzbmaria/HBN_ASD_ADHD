# HBN ASD/ADHD Hyperalignment Pipeline

Hyperalignment pipeline for HBN (Healthy Brain Network) CIFTI dtseries data with support for both local execution (Mac/Ubuntu) and PBS cluster deployment.

## ğŸš€ Quick Start

### 1. Build Docker Image
```bash
./docker-build.sh
```

### 2. Test with Sample Subjects
```bash
export DATA_ROOT=/path/to/your/data
./test_pipeline.sh
```

### 3. Run Full Pipeline
```bash
# Local (Mac/Ubuntu)
./local_scripts/run_full_pipeline.sh

# PBS Cluster
./docker-to-singularity.sh
# Then follow PBS guide
```

## ğŸ“š Documentation

Choose your path:

### ğŸ”° Getting Started
- **[BUILD_AND_TEST.md](BUILD_AND_TEST.md)** â† **START HERE**
  - How to build Docker image
  - How to test with small subset of subjects
  - Validation and troubleshooting

### ğŸ’» Local Execution (Mac/Ubuntu)
- **[LOCAL_EXECUTION_GUIDE.md](LOCAL_EXECUTION_GUIDE.md)**
  - Run pipeline on your Mac or Ubuntu server
  - Resource requirements and optimization
  - Step-by-step instructions

### â˜ï¸ PBS Cluster Execution
- **[DOCKER_PBS_README.md](DOCKER_PBS_README.md)**
  - Convert Docker to Singularity
  - PBS job scripts
  - Cluster deployment guide

### âš™ï¸ Configuration
- **[hyperalignment_scripts/CONFIG_README.md](hyperalignment_scripts/CONFIG_README.md)**
  - Centralized configuration system
  - How to customize parameters

### âš¡ Quick Reference
- **[QUICKSTART.md](QUICKSTART.md)**
  - Quick command reference
  - Common configurations

## ğŸ¯ What This Pipeline Does

```
Raw CIFTI dtseries files (*.dtseries.nii)
    â†“
[1] Parcellation (Glasser Atlas)
    â†’ Creates parcellated timeseries (*.ptseries.nii)
    â†“
[2] Hyperalignment (Python 2 + PyMVPA2)
    â†’ Learns alignment transformations
    â†’ Creates aligned timeseries
    â†“
[3] Build Connectomes (Python 3)
    â†’ Generates connectivity matrices
    â†“
Output: Connectivity matrices (.npy files)
```

## ğŸ³ Docker Container

The Docker container includes:
- **Python 2.7**: For PyMVPA2 hyperalignment
- **Python 3.8**: For connectome analysis
- **PyMVPA2**: Hyperalignment algorithm
- **Scientific Stack**: numpy, scipy, nibabel, pandas, scikit-learn
- **Connectome Workbench**: CIFTI processing tools

## ğŸ“‚ Project Structure

```
HBN_ASD_ADHD/
â”œâ”€â”€ Dockerfile                          # Container definition
â”œâ”€â”€ docker-build.sh                     # Build Docker image
â”œâ”€â”€ test_pipeline.sh                    # Test with sample subjects
â”‚
â”œâ”€â”€ local_scripts/                      # Local execution (Mac/Ubuntu)
â”‚   â”œâ”€â”€ run_parcellation.sh
â”‚   â”œâ”€â”€ run_hyperalignment_single.sh
â”‚   â”œâ”€â”€ run_hyperalignment_parallel.sh
â”‚   â”œâ”€â”€ run_build_connectomes.sh
â”‚   â””â”€â”€ run_full_pipeline.sh
â”‚
â”œâ”€â”€ pbs_scripts/                        # PBS cluster execution
â”‚   â”œâ”€â”€ pbs_parcellation.sh
â”‚   â”œâ”€â”€ pbs_hyperalignment_array.sh
â”‚   â”œâ”€â”€ pbs_build_connectomes.sh
â”‚   â””â”€â”€ submit_pipeline.sh
â”‚
â”œâ”€â”€ hyperalignment_scripts/             # Pipeline code
â”‚   â”œâ”€â”€ config.sh                       # Centralized configuration
â”‚   â”œâ”€â”€ read_config.py                  # Config reader (Python 2/3)
â”‚   â”œâ”€â”€ utils.py                        # Utility functions
â”‚   â”œâ”€â”€ run_hyperalignment.py           # Hyperalignment (Python 2)
â”‚   â”œâ”€â”€ build_aa_connectomes.py         # Build connectomes (Python 3)
â”‚   â”œâ”€â”€ build_CHA_connectomes.py        # CHA connectomes (Python 3)
â”‚   â””â”€â”€ apply_parcellation.sh           # Parcellation script
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ BUILD_AND_TEST.md               # Build and test guide
    â”œâ”€â”€ LOCAL_EXECUTION_GUIDE.md        # Local execution guide
    â”œâ”€â”€ DOCKER_PBS_README.md            # PBS cluster guide
    â”œâ”€â”€ QUICKSTART.md                   # Quick reference
    â””â”€â”€ CONFIG_README.md                # Configuration docs
```

## ğŸ”§ Data Directory Structure

Your data should be organized as:

```
/path/to/your/data/
â”œâ”€â”€ HBN_CIFTI/                          # Input dtseries files
â”‚   â”œâ”€â”€ sub-NDARAA123_task-rest_run-1_nogsr_Atlas_s5.dtseries.nii
â”‚   â”œâ”€â”€ sub-NDARAA456_task-rest_run-1_nogsr_Atlas_s5.dtseries.nii
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ diagnosis_summary/                  # Subject metadata (optional)
â”‚   â””â”€â”€ matched_subjects_diagnosis_mini.csv
â”‚
â”œâ”€â”€ hyperalignment_input/               # Created by pipeline
â”‚   â””â”€â”€ glasser_ptseries/
â”‚
â””â”€â”€ connectomes/                        # Created by pipeline
    â”œâ”€â”€ fine/                           # Connectivity matrices
    â””â”€â”€ hyperalignment_output/          # Aligned timeseries + mappers
```

## ğŸ’¡ Common Use Cases

### Test with 5 subjects (auto-selected)
```bash
export DATA_ROOT=/path/to/your/data
./test_pipeline.sh
```

### Test with specific subjects
```bash
export DATA_ROOT=/path/to/your/data
export TEST_SUBJECTS="sub-NDARAA123 sub-NDARAA456 sub-NDARAA789"
./test_pipeline.sh
```

### Run locally on first 10 parcels
```bash
export DATA_ROOT=/path/to/your/data
export START_PARCEL=1
export END_PARCEL=10
./local_scripts/run_hyperalignment_parallel.sh
```

### Run single parcel for testing
```bash
export DATA_ROOT=/path/to/your/data
./local_scripts/run_hyperalignment_single.sh 1 full
```

### Run full pipeline locally (all subjects, all parcels)
```bash
export DATA_ROOT=/path/to/your/data
./local_scripts/run_full_pipeline.sh
```

### Deploy to PBS cluster
```bash
# 1. Build and test locally first
./docker-build.sh
./test_pipeline.sh

# 2. Convert to Singularity
./docker-to-singularity.sh

# 3. Transfer to cluster
scp hyperalignment.sif your-cluster:/project/hyperalignment/
scp -r pbs_scripts your-cluster:/project/hyperalignment/

# 4. Submit on cluster
cd /project/hyperalignment
export DATA_ROOT=/scratch/username/HBN_data
./pbs_scripts/submit_pipeline.sh
```

## ğŸ“ Learning Path

1. **First Time Users**: Start with [BUILD_AND_TEST.md](BUILD_AND_TEST.md)
2. **Local Execution**: Read [LOCAL_EXECUTION_GUIDE.md](LOCAL_EXECUTION_GUIDE.md)
3. **Cluster Execution**: Read [DOCKER_PBS_README.md](DOCKER_PBS_README.md)
4. **Customize Settings**: Read [CONFIG_README.md](hyperalignment_scripts/CONFIG_README.md)

## âš™ï¸ Features

- âœ… **Centralized Configuration**: Single `config.sh` for all parameters
- âœ… **Dual Python Support**: Python 2 (hyperalignment) + Python 3 (analysis)
- âœ… **Flexible Deployment**: Local (Docker) or Cluster (Singularity/PBS)
- âœ… **Test Mode**: Test with subset of subjects before full run
- âœ… **Resume Capability**: Pipeline can resume from interruptions
- âœ… **Parallel Processing**: Optimized for multi-core systems
- âœ… **Comprehensive Logging**: Detailed logs for debugging
- âœ… **Validation**: Automatic output validation

## ğŸ” Resource Requirements

### Local Execution (Mac/Ubuntu)
- **CPU**: 4-8 cores minimum (more is better)
- **RAM**: 16GB minimum, 32GB+ recommended
- **Disk**: 50GB+ free space
- **Time**: Days for all 360 parcels

### PBS Cluster Execution
- **Per-job resources**: 24 CPUs, 128GB RAM, 24h walltime
- **Time**: Hours for all 360 parcels (with array jobs)

## ğŸ› Troubleshooting

See the documentation for detailed troubleshooting:
- [BUILD_AND_TEST.md](BUILD_AND_TEST.md#troubleshooting)
- [LOCAL_EXECUTION_GUIDE.md](LOCAL_EXECUTION_GUIDE.md#troubleshooting)
- [DOCKER_PBS_README.md](DOCKER_PBS_README.md#troubleshooting)

## ğŸ“Š Pipeline Components

### Parcellation (`apply_parcellation.sh`)
- Uses Connectome Workbench `wb_command`
- Applies Glasser atlas to dtseries files
- Creates ptseries files

### Hyperalignment (`run_hyperalignment.py`)
- Python 2 + PyMVPA2
- Learns subject-to-template transformations
- Supports full and split-half modes
- Processes each parcel independently

### Connectomes (`build_aa_connectomes.py`, `build_CHA_connectomes.py`)
- Python 3 + scipy
- Builds connectivity matrices
- Multiple connectome types supported

## ğŸ¤ Contributing

For issues or questions:
1. Check documentation in this repository
2. Review logs in `logs/` directory
3. Test with `./test_pipeline.sh`
4. Create an issue with details

## ğŸ“ Citation

If you use this pipeline, please cite:
- PyMVPA2 for hyperalignment
- HCP Workbench for CIFTI processing
- Healthy Brain Network (HBN) for the dataset

## ğŸ“„ License

[Your License Here]

---

## Quick Command Reference

```bash
# Build
./docker-build.sh

# Test (5 subjects, 3 parcels)
export DATA_ROOT=/path/to/data
./test_pipeline.sh

# Test (specific subjects)
export TEST_SUBJECTS="sub-XXX sub-YYY sub-ZZZ"
./test_pipeline.sh

# Run locally (full pipeline)
./local_scripts/run_full_pipeline.sh

# Run locally (single parcel)
./local_scripts/run_hyperalignment_single.sh 1 full

# Convert for cluster
./docker-to-singularity.sh

# Interactive shell
./docker-run.sh
```

---

**Need help?** Start with [BUILD_AND_TEST.md](BUILD_AND_TEST.md) for step-by-step instructions!
